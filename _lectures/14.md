---
type: lecture

date: 2025-03-06T02:00:00+5:30

title: "Seq-to-Seq Attention and Transformers"

tldr: "seq2seq attention; variants of attention; introduction to positional encoding"

hide_from_announcments: false

thumbnail: /_images/lecs/01.png

links: 
    - url: /_images/lecs/lec14a.pdf
      name: slides (Seq-to-Seq Attention)
    - url: /_images/lecs/lec14b.pdf
      name: slides (Transformers)
---
**Supplementary Material:**
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
