---
type: lecture

date: 2025-02-20T02:00:00+5:30

title: "Neural Language Models"

tldr: "RNNs, training RNNs, backpropagation through time"

hide_from_announcments: true

thumbnail: /_images/lecs/01.png

links: 
    - url: /_images/lecs/lec12.pdf
      name: slides
---
**Supplementary Material:**
- [Textbook: Chapter 5 -- Introduction to Large Language Models, Tanmoy Chakraborty](https://www.amazon.in/Introduction-Large-Language-Models-Generative/dp/936386474X?crid=3EEJDPN3KFTX2&dib=eyJ2IjoiMSJ9.4K1aiA--SOAJiVgp0r98fQ._Dsmj2zd6yN1P6aoDLSXZJ1cdi7H9OnVvi9S_Edmw_E&dib_tag=se&keywords=tanmoy+chakraborty%2C+introduction+to+large+language+models&qid=1739206737&sprefix=tanmoy%2Caps%2C285&sr=8-1)

- [Reading](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/readings/cs224n-2019-notes05-LM_RNN.pdf)
- [Vanishing Gradient proof](https://arxiv.org/pdf/1211.5063)
- [Blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
