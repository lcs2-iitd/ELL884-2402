---
type: lecture

date: 2025-03-27T02:00:00+5:30

title: "Pretraining and Tokenization Strategies"

tldr: "Pre-Training BART, T5, GPT and LLaMa family, causal masking, sub-word tokenization, Byte-Pair Encoding, wordPiece tokenization, unigram language model tokenization"

hide_from_announcments: true

thumbnail: /_images/lecs/01.png

links: 
    - url: /_images/lecs/lec17a.pdf
      name: slides (Pretraining Strategies)
    - url: /_images/lecs/lec17b.pdf
      name: slides (Tokenization Strategies)
---
**Supplementary Material**
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144v2)
