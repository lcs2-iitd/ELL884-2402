---
type: lecture

date: 2025-04-12T02:00:00+5:30

title: "Alignment of Language Models"

tldr: "LLM training stages and alignment; limits of instruction tuning; RLHF with human or AI feedback; reward model using Bradley-Terry preferences; REINFORCE and gradient tricks; Q-function and advantage estimation; PPO for stable policy optimization."

hide_from_announcments: false

thumbnail: /_images/lecs/01.png

links: 
    - url: /_images/lecs/lec20.pdf
      name: slides
---
**Supplementary Material:**
- [Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)
- [Reinforcement Learning for Language Models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073)
