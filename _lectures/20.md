---
type: lecture

date: 2025-04-12T02:00:00+5:30

title: "Alignment of Language Models"

tldr: "LLM training stages and alignment; limits of instruction tuning; RLHF with human or AI feedback; reward model using Bradley-Terry preferences; REINFORCE and gradient tricks; Q-function and advantage estimation; PPO for stable policy optimization."

hide_from_announcments: false

thumbnail: /_images/lecs/01.png

links: 
    - url: /_images/lecs/lec20.pdf
      name: slides
---
